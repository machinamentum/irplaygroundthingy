#include "general.h"
#include "linker_object.h"
#include "ir.h"
#include "aarch64.h"

using namespace josh;

enum Integer_Register : u8 {
    GP_R0 = 0,
    GP_R7 = 7,
    GP_R9 = 9,
    GP_R29 = 29,
    GP_R30 = 30,
    GP_MAX = GP_R30,
    SP = 31,
    // PC = 32,
};

#define OP_GROUP(op) (((op) & 0xFF) << 25)
#define OP0_RESERVED      OP_GROUP(0b0000)
#define _OP0_UNALLOCATED  OP_GROUP(0b0001)
#define OP0_SVE           OP_GROUP(0b0010)
#define _OP0_UNALLOCATED2 OP_GROUP(0b0011)
#define OP0_DATA_IMM      OP_GROUP(0b1000) // 100x
#define OP0_BRANCH        OP_GROUP(0b1010) // 101x
#define OP0_LOAD_STORE    OP_GROUP(0b0100) // x1x0
#define OP0_DATA_REG      OP_GROUP(0b0101) // x101
#define OP0_SCALAR_FLOAT  OP_GROUP(0b0111)


#define DATA_IMM_SUBOP0(op0) (((op0) & 0b111) << 23)
#define DATA_IMM_ADD_SUB        DATA_IMM_SUBOP0(0b010)
#define DATA_IMM_ADD_SUB_W_TAGS DATA_IMM_SUBOP0(0b011)
#define DATA_IMM_ADR            DATA_IMM_SUBOP0(0b000)

#define DATA_IMM_ADRP (1 << 31)
#define DATA_IMM_ADR_IMMHI(imm) ((((imm) >> 2) & 0x7FFFF) << 5) // 19 bits >.>
#define DATA_IMM_ADR_IMMLO(imm) (((imm) & 0b11) << 29)
#define DATA_IMM_ADR_Rd(Rd) ((Rd) & 0x1F)

#define BRANCH_SUBOP0(op0) (((op0) & 0b111) << 29)
#define BRANCH_SUBOP1(op1) (((op1) & 0x3FFF) << 12)
#define BRANCH_SUBOP2(op2) (((op2) & 0x1F))

#define BRANCH_UNC_REG_CLASS (OP0_BRANCH | BRANCH_SUBOP0(0b110) | (1 << 25))
#define BRANCH_UNC_REG_OPC(opc) (((opc) & 0xF) << 21)
#define BRANCH_UNC_REG_OP2(op2) (((op2) & 0x1F) << 16)
#define BRANCH_UNC_REG_OP3(op3) (((op3) & 0x3F) << 10)
#define BRANCH_UNC_REG_Rn(Rn)   (((Rn) & 0x1F) << 5)
#define BRANCH_UNC_REG_OP4(op4) ((op4) & 0x1F)

#define BRANCH_UNC_IMM_IMM26(imm26) ((imm26) & 0x3FFFFFF)
#define BRANCH_UNC_IMM_BL (1 << 31)

#define EXCEPTION_CLASS (OP0_BRANCH | BRANCH_SUBOP0(0b110))
#define EXCEPTION_OPC(opc) (((opc) & 0b111) << 21)
#define EXCEPTION_IMM16(imm16) (((imm16) & 0xFFFF) << 5)
#define EXCEPTION_OP2(op2) (((op2) & 0b111) << 2)
#define EXPCETION_LL(ll) ((ll) & 0b11)

#define LOAD_STORE_OP0(op0) (((op0) & 0xF) << 28)
#define LOAD_STORE_OP1(op1) (((op1) & 1) << 26)
#define LOAD_STORE_OP2(op2) (((op2) & 0b11) << 23)
#define LOAD_STORE_OP3(op3) (((op3) & 0x3F) << 16)
#define LOAD_STORE_OP4(op4) (((op4) & 0b11) << 10)

// Load/Store Pair Register
#define LSPPI_IMM7(imm7) (((imm7) & 0x7F) << 15)
#define LSPPI_Rt2(Rt2) (((Rt2) & 0x1F) << 10)
#define LSPPI_Rn(Rn) (((Rn) & 0x1F) << 5)
#define LSPPI_Rt(Rt) (((Rt) & 0x1F))


// sf = 64-bit if 1, 32-bit otherwise
// op = sub if 1, add otherwise
// S  = set flags if 1
#define SF_OP_S(sf, op, S) ((((sf) & 1) << 31) | (((op) & 1) << 30) | (((S) & 1) << 29))
#define Rn(reg) (reg << 5)
#define Rd(reg) (reg)

// ADD/S, SUB/S support an optional shift of imm12 in bit 22

static
void sub_imm12_from_reg64(Data_Buffer *dataptr, u8 dst, u8 src, u16 imm12, u8 size = 8, bool set_flags = false) {
    // imm12 is zext'd to 'datasize' (8 or 4 bytes)
    assert(imm12 & 0x0FFF);
    dataptr->append<u32>(OP0_DATA_IMM | DATA_IMM_ADD_SUB | SF_OP_S(size == 8 ? 1 : 0, 1, set_flags ? 1 : 0) | Rd(dst) | Rn(src) | (imm12 << 10));
}

static
void add_imm12_to_reg64(Data_Buffer *dataptr, u8 dst, u8 src, u16 imm12, u8 size = 8, bool set_flags = false) {
    // imm12 is zext'd to 'datasize' (8 or 4 bytes)
    assert((imm12 & ~0x0FFF) == 0);
    dataptr->append<u32>(OP0_DATA_IMM | DATA_IMM_ADD_SUB | SF_OP_S(size == 8 ? 1 : 0, 0, set_flags ? 1 : 0)  | Rd(dst) | Rn(src) | (imm12 << 10));
}

static
void debugbreak(Data_Buffer *dataptr) {
    // TODO I am not really sure if the immediate in the brk instruction is the same for all software breakpoints on
    // all ARM64 operating systems. #0xF000 is what is generated by Clang on macOS
    dataptr->append<u32>(EXCEPTION_CLASS | EXCEPTION_OPC(0b001) | EXCEPTION_IMM16(0xF000));
}

static
void ret(Data_Buffer *dataptr, Integer_Register Rn = GP_R30) {
    dataptr->append<u32>(BRANCH_UNC_REG_CLASS | BRANCH_UNC_REG_OPC(0b0010) | BRANCH_UNC_REG_OP2(0b11111) | BRANCH_UNC_REG_Rn(Rn));
}

// PC-relative address calculation
// Immediate is a +/- 1MB offset from PC
static
void adr(Data_Buffer *dataptr, s32 immediate, u8 Rd) {
    assert(fits_into_bits(immediate, 21));
    dataptr->append<u32>(OP0_DATA_IMM | DATA_IMM_SUBOP0(0b000) | DATA_IMM_ADR_IMMHI(immediate) | DATA_IMM_ADR_IMMLO(immediate) | DATA_IMM_ADR_Rd(Rd));
}

// PC-relative address calculation
// Immediate is a +/- 4GB offset from PC
// Calculated by PC += sext(immediate21) * 4096 /* page size */ 
static
void adrp(Data_Buffer *dataptr, s32 immediate, u8 Rd) {
    assert(fits_into_bits(immediate, 21));
    dataptr->append<u32>(DATA_IMM_ADRP | OP0_DATA_IMM | DATA_IMM_SUBOP0(0b000) | DATA_IMM_ADR_IMMHI(immediate) | DATA_IMM_ADR_IMMLO(immediate) | DATA_IMM_ADR_Rd(Rd));
}

static
void b(Data_Buffer *dataptr, s32 imm26) {
    assert(fits_into_bits(imm26, 26));
    dataptr->append<u32>(OP0_BRANCH | BRANCH_UNC_IMM_IMM26(imm26));
}

static
void bl(Data_Buffer *dataptr, s32 imm26) {
    assert(fits_into_bits(imm26, 26));
    dataptr->append<u32>(OP0_BRANCH | BRANCH_UNC_IMM_BL | BRANCH_UNC_IMM_IMM26(imm26));
}

static
void br(Data_Buffer *dataptr, u8 Rn) {
    dataptr->append<u32>(BRANCH_UNC_REG_CLASS | BRANCH_UNC_REG_OPC(0b0000) | BRANCH_UNC_REG_OP2(0b11111) | BRANCH_UNC_REG_Rn(Rn));
}

static
void blr(Data_Buffer *dataptr, u8 Rn) {
    dataptr->append<u32>(BRANCH_UNC_REG_CLASS | BRANCH_UNC_REG_OPC(0b0001) | BRANCH_UNC_REG_OP2(0b11111) | BRANCH_UNC_REG_Rn(Rn));
}

static
void _load_store_pair_impl(Data_Buffer *dataptr, u8 Rt, u8 Rt2, u8 Rn, s32 imm7, bool load, bool post_index) {
    assert((imm7 & 0x7) == 0); // must be a multiple of 8
    imm7 >>= 3;
    assert(fits_into_bits(imm7, 7));
    const u32 V = (1 << 26); // V bit turns this into a floating-point/SIMD variant
    const u32 L = (1 << 22); // L bit = load if 1, store otherwise

    // TODO there's also a 'signed offset' variant that is denoted by op2 = 0b10
    // I am not really sure what the difference is but using the signed offset variant
    // in place of pre-index seems to crash basic programs on return :/

    // There's three addressing modes available:
    // signed offset (0b10): calculates address by adding together Rn and imm7
    // pre-index     (0b11): calculates address by adding together Rn and imm7, then stores the new address in Rn
    // post-index     (0b11): calculates address as the value currently in Rn, then adds Rn and imm7 together and stores the new address in Rn
    // The latter two effectively let you implement a 2-register push-pop using pre-index stp followed up by a post-index ldp.
    u32 op2 = LOAD_STORE_OP2(0b11);
    if (post_index)
        op2 = LOAD_STORE_OP2(0b01);

    dataptr->append<u32>(OP0_LOAD_STORE | (load ? L : 0) | LOAD_STORE_OP0(0b1010) | op2 | LSPPI_Rt(Rt) | LSPPI_Rt2(Rt2) | LSPPI_Rn(Rn) | LSPPI_IMM7(imm7));
}

static
void stp(Data_Buffer *dataptr, u8 Rt, u8 Rt2, u8 Rn, s32 imm7, bool post_index = false) {
    _load_store_pair_impl(dataptr, Rt, Rt2, Rn, imm7, false, post_index);
}

static
void ldp(Data_Buffer *dataptr, u8 Rt, u8 Rt2, u8 Rn, s32 imm7, bool post_index = false) {
    _load_store_pair_impl(dataptr, Rt, Rt2, Rn, imm7, true, post_index);
}

static
Register *get_free_register(AArch64_Emitter *emitter) {
    for (auto &reg : emitter->register_usage) {
        if (reg.is_free) {
            reg.is_free = false;
            return &reg;
        }
    }

    return nullptr;
}

/*
Register *get_free_xmm_register(AArch64_Emitter *emitter) {
    for (auto &reg : emitter->xmm_usage) {
        if (reg.is_free) {
            reg.is_free = false;
            return &reg;
        }
    }

    return nullptr;
}
*/

void maybe_spill_register(AArch64_Emitter *emitter, Register *reg) {
    if (reg->currently_holding_result_of_instruction) {
        auto inst = reg->currently_holding_result_of_instruction;
        inst->result_stored_in = nullptr;

        if (inst->result_spilled_onto_stack == 0 && inst->uses) {
            emitter->stack_size += 8; // @TargetInfo
            inst->result_spilled_onto_stack = -emitter->stack_size;

            // TODO
            // move_register_value_to_memory(&emitter->function_buffer, reg->machine_reg, addr_register_disp(RBP, inst->result_spilled_onto_stack), inst->value_type);
        }

        reg->currently_holding_result_of_instruction = nullptr;
    }

    reg->is_free = true;
}

Register *claim_register(AArch64_Emitter *emitter, Register *reg, Value *claimer) {
    maybe_spill_register(emitter, reg);

    if (claimer) {
        reg->currently_holding_result_of_instruction = claimer;
        claimer->result_stored_in = reg;
    }

    reg->is_free = false;
    reg->used    = true;
    return reg;
}

static
void free_register(Register *reg) {
    reg->currently_holding_result_of_instruction = nullptr;
    reg->is_free = true;
}

Register *get_free_or_suggested_register(AArch64_Emitter *emitter, u8 suggested_register, bool force_use_suggested, Value *inst) {
    Register *reg = nullptr;

    if (!force_use_suggested) {
        reg = get_free_register(emitter);
        if (!reg) {

            u32 uses = U32_MAX;
            Register *regi;
            for (auto &reg : emitter->register_usage) {
                if (reg.machine_reg == SP) continue;

                if (reg.currently_holding_result_of_instruction) {
                    auto inst = reg.currently_holding_result_of_instruction;

                    if (inst->uses < uses) {
                        uses = inst->uses;
                        regi = &reg;
                    }
                }
            }

            reg = regi;
        }
    }

    if (!reg) reg = &emitter->register_usage[suggested_register];

    maybe_spill_register(emitter, reg);
    if (inst) return claim_register(emitter, reg, inst);

    
    return reg;
}

/*
u8 load_instruction_result(AArch64_Emitter *emitter, Value *inst, u8 suggested_register, bool force_use_suggested) {
    if (auto reg = inst->result_stored_in) {
        assert(reg->currently_holding_result_of_instruction == inst);
        return reg->machine_reg;
    } else {
        assert(!inst->result_stored_in);
        reg = get_free_or_suggested_register(emitter, suggested_register, force_use_suggested, inst);

        assert(inst->result_spilled_onto_stack != 0);
        move_memory_value_to_register(&emitter->function_buffer, reg->machine_reg, addr_register_disp(RBP, inst->result_spilled_onto_stack), inst->value_type);
        return reg->machine_reg;
    }
}
*/

static
void load_symbol_address(AArch64_Emitter *emitter, Section *code_section, u32 symbol_index, u32 offset, u8 machine_reg) {
    adrp(&emitter->function_buffer, 0, machine_reg);
    {
        Relocation reloc;
        reloc.type = Relocation::RIP_DATA;
        reloc.offset = emitter->function_buffer.size() - 4;
        reloc.symbol_index = symbol_index;
        reloc.size = 4;
        code_section->relocations.push_back(reloc);
    }

    // TODO since we only have 12 bits to work with, we can only address 1
    // page into the data section. We can remedy this by introducing another
    // add with the shift bit set, which shifts up the imm12 by 12 bits, allowing
    // us to address offets up to 0x1000000 (16MB). The long term solution is likely
    // to create temporary symbols for most items in the data section :/
    // ... or maybe we can add an addend to the adrp relocation ? which would
    // give us +- 4GB
    add_imm12_to_reg64(&emitter->function_buffer, machine_reg, machine_reg, 0);
    {
        Relocation reloc;
        reloc.type = Relocation::PAGEOFFSET;
        reloc.offset = emitter->function_buffer.size() - 4;
        reloc.symbol_index = symbol_index;
        reloc.size = 4;
        reloc.addend = offset;
        code_section->relocations.push_back(reloc);
    }
}

u8 emit_load_of_value(AArch64_Emitter *emitter, Linker_Object *object, Section *code_section, Value *value, u8 suggested_register = GP_R0, bool force_use_suggested = false) {
    if (value->type == VALUE_CONSTANT) {
        auto constant = static_cast<Constant *>(value);

        Register *reg = get_free_or_suggested_register(emitter, suggested_register, force_use_suggested, nullptr);
        reg->is_free = false;

        Section *data_section = emitter->data_section;

        if (constant->constant_type == Constant::STRING) {
            u32 data_sec_offset = data_section->data.size();

            // copy the string characters into the data section
            assert(constant->string_value.length() <= U32_MAX);

            // TODO can we avoid a string copy by copying the string table into the data section later on and then fixing up all the offsets ?
            if (auto str_id = emitter->string_table.intern(constant->string_value)) {
                auto &entry = emitter->string_table.lookup(str_id);
                if (entry.data_sec_offset == U32_MAX) { // New entry
                    size_t length = static_cast<u32>(constant->string_value.length());
                    void *data_target = data_section->data.allocate_bytes_unaligned(length);
                    memcpy(data_target, constant->string_value.data(), length);
                    data_section->data.append_byte(0);
                    entry.data_sec_offset = data_sec_offset;
                } else {
                    data_sec_offset = entry.data_sec_offset;
                }
            } else {
                // Empty string, assume at data section offset 0.
                data_sec_offset = 0;
            }

            load_symbol_address(emitter, code_section, data_section->symbol_index, data_sec_offset, reg->machine_reg);

        } else if (constant->constant_type == Constant::INTEGER) {
            // move_imm_to_reg_or_clear(&emitter->function_buffer, constant->integer_value, reg->machine_reg);
        } else if (constant->constant_type == Constant::FLOAT) {
            /*
            Register *xmm = get_free_xmm_register(emitter);
            if (!xmm) {
                xmm = claim_register(emitter, &emitter->xmm_usage[XMM0], value);
            } else {
                xmm = claim_register(emitter, xmm, value);
            }

            // copy the float bytes into the data section
            if (constant->value_type->size == 8) {
                double *data_target = data_section->data.allocate<double>();
                *data_target = constant->float_value;
            } else if (constant->value_type->size == 4) {
                float *data_target = data_section->data.allocate<float>();
                *data_target = (float)constant->float_value;
            } else {
                assert(false);
            }

            u32 data_sec_offset = data_section->data.size() - constant->value_type->size;
            assert(data_sec_offset >= 0);

            if (object->use_absolute_addressing) {
                move_imm64_to_reg64(&emitter->function_buffer, data_sec_offset, reg->machine_reg, 8);
    
                Relocation reloc;
                reloc.is_for_rip_call = false;
                reloc.offset = emitter->function_buffer.size() - 8;
                reloc.symbol_index = data_section->symbol_index;
                reloc.size = 8;
                reloc.addend = static_cast<u64>(data_sec_offset);

                code_section->relocations.push_back(reloc);

                move_memory_to_xmm(&emitter->function_buffer, xmm->machine_reg, addr_register_disp(reg->machine_reg), constant->value_type->size);
            } else {
                s32 *value = move_memory_to_xmm(&emitter->function_buffer, xmm->machine_reg, addr_register_disp(RBP), constant->value_type->size);
                *value = static_cast<s32>(data_sec_offset);

                Relocation reloc;
                reloc.is_for_rip_call = false;
                reloc.is_rip_relative = true;
                reloc.offset = emitter->function_buffer.size() - 4;
                reloc.symbol_index = data_section->symbol_index;
                reloc.size = 4;

                code_section->relocations.push_back(reloc);
            }

            free_register(reg);
            return xmm->machine_reg;
            */
        }

        return reg->machine_reg;
    } /*else if (value->type == VALUE_BASIC_BLOCK) {
        Basic_Block *block = static_cast<Basic_Block *>(value);

        Register *reg = get_free_or_suggested_register(emitter, suggested_register, force_use_suggested, nullptr);
        reg->is_free = false;

        s32 *value = lea_rip_relative_into_reg64(&emitter->function_buffer, reg->machine_reg);

        auto offset = emitter->function_buffer.size() - 4;
        block->text_locations_needing_addr_fixup.push_back(offset);

        block->text_ptrs_for_fixup.push_back((u32 *)value);
        return reg->machine_reg;
    } else if (value->type == INSTRUCTION_ALLOCA) {
        auto _alloca = static_cast<Instruction_Alloca *>(value);

        if (_alloca->result_stored_in) return _alloca->result_stored_in->machine_reg;
        if (_alloca->result_spilled_onto_stack != 0) return load_instruction_result(emitter, _alloca, suggested_register, force_use_suggested);

        Register *reg = get_free_or_suggested_register(emitter, suggested_register, force_use_suggested, _alloca);

        assert(_alloca->stack_offset != 0);
        lea_into_reg64(&emitter->function_buffer, reg->machine_reg, addr_register_disp(RBP, _alloca->stack_offset));
        return reg->machine_reg;
    } else if (value->type == VALUE_ARGUMENT) {
        auto arg = static_cast<Argument *>(value);
        if (arg->copied_to_stack_offset) {
            Address_Info info = addr_register_disp(RBP, arg->copied_to_stack_offset);
            Register *reg = get_free_or_suggested_register(emitter, suggested_register, force_use_suggested, nullptr);
            lea_into_reg64(&emitter->function_buffer, reg->machine_reg, info);
            return reg->machine_reg;
        }

        return load_instruction_result(emitter, value, suggested_register, force_use_suggested);
    } else if (value->type >= INSTRUCTION_FIRST && value->type <= INSTRUCTION_LAST) {
        auto inst = static_cast<Instruction *>(value);
        return load_instruction_result(emitter, inst, suggested_register, force_use_suggested);
    }

    assert(false);
    */
    return 0;
}


/*
u8 maybe_get_instruction_register(Value *value) {
    if (Instruction *inst = is_instruction(value)) {
        if (inst->result_stored_in) return inst->result_stored_in->machine_reg;
    }

    return 0xFF;
}
*/

/*
Address_Info get_address_value_of_pointer(AArch64_Emitter *emitter, Linker_Object *object, Section *code_section, Value *value, u8 suggested_register = RAX) {
    assert(value->value_type->type == Type::POINTER);

    if (value->type == INSTRUCTION_ALLOCA) {
        auto _alloca = static_cast<Instruction_Alloca *>(value);
        return addr_register_disp(RBP, _alloca->stack_offset);
    }
    else if (value->type == VALUE_ARGUMENT) {
        auto arg = static_cast<Argument *>(value);
        if (arg->copied_to_stack_offset)
            return addr_register_disp(RBP, arg->copied_to_stack_offset);
    }

// We could theoretically do this and save some instructions
// but this ends up generating a large amount of bytes
// due to having to write the displacement bytes. I don't yet know
// if code in that form is faster due to fewer instructions or due
// to smaller code size. Note also that that emit_load_of_value of
// gep->index could also generate additional instructions...
#if 0
    else if (value->type == INSTRUCTION_GEP) {
        Instruction_GEP *gep = static_cast<Instruction_GEP *>(value);

        u32 size = gep->pointer_value->value_type->pointer_to->size;

        if (size > 8) {
            // we cant claim ownership of the index register so the result of this calculation
            // would be nontrivial, just get the stored instruction result
            u8 machine_reg = emit_load_of_value(emitter, object, code_section, value, suggested_register);
            return addr_register_disp(machine_reg);
        }

        u8 target = maybe_get_instruction_register(gep->index);

        Address_Info source = get_address_value_of_pointer(emitter, object, code_section, gep->pointer_value, (target == RAX) ? RCX : RAX);
        // gep->pointer_value->uses--;

        if (target == 0xFF) target = emit_load_of_value(emitter, object, code_section, gep->index,         (source.machine_reg == RAX) ? RCX : RAX);
        // gep->index->uses--;

        assert(source.machine_reg != target);

        Address_Info info;
        info.machine_reg = RSP; // SIB
        info.disp        = source.disp;
        info.base_reg    = source.machine_reg;
        info.index_reg   = target;
        info.scale       = (u8) size;

        return info;
    }
#endif

    {
        u8 machine_reg = emit_load_of_value(emitter, object, code_section, value, suggested_register);
        return addr_register_disp(machine_reg);
    }
}
*/

static
u8 emit_instruction(AArch64_Emitter *emitter, Linker_Object *object, Function *function, Basic_Block *current_block, Section *code_section, Instruction *inst) {
    switch (inst->type) {
        case INSTRUCTION_ALLOCA: {
            auto _alloca = static_cast<Instruction_Alloca *>(inst);

            // Register *reg = get_free_or_suggested_register(emitter, RAX, false, inst);

            assert(_alloca->stack_offset != 0);
            // lea_into_reg64(&emitter->function_buffer, reg->machine_reg, addr_register_disp(RBP, -_alloca->stack_offset));
            return 0;
        }

        case INSTRUCTION_STORE: {
            auto store = static_cast<Instruction_Store *>(inst);

            // TODO

            store->store_target->uses--;
            store->source_value->uses--;
            break;
        }

        case INSTRUCTION_LOAD: {
            auto load = static_cast<Instruction_Load *>(inst);

            // TODO

            load->pointer_value->uses--;

            return 0;
        }

        case INSTRUCTION_GEP: {
            Instruction_GEP *gep = static_cast<Instruction_GEP *>(inst);

            gep->pointer_value->uses--;
            gep->index->uses--;

            return 0;
        }

        case INSTRUCTION_DIV: {
            auto div = static_cast<Instruction_Div *>(inst);

            // TODO

            div->lhs->uses--;
            div->rhs->uses--;
            return 0;
        }

        case INSTRUCTION_ADD:
        case INSTRUCTION_SUB:
        case INSTRUCTION_MUL: {
            auto add = static_cast<Instruction_Add *>(inst);
            
            // TODO

            add->lhs->uses--;
            add->rhs->uses--;

            return 0;
        }

        case INSTRUCTION_CALL: {
            auto call = static_cast<Instruction_Call *>(inst);
            auto function_target = static_cast<Function *>(call->call_target);
            auto func_type = static_cast<Function_Type *>(function_target->value_type);

            assert(func_type->type == Type::FUNCTION);

            if (function_target->intrinsic_id) {
                switch (function_target->intrinsic_id) {
                    case Function::NOT_INTRINSIC:
                        assert(false);
                        break;
                    case Function::DEBUG_BREAK:
                        debugbreak(&emitter->function_buffer);
                        break;
                }

                return 0;
            }


            // TODO necessary on arm64?
            if (object->target.is_win32()) {
                // shadow space for the callee to spill registers...
                emitter->largest_call_stack_adjustment += 32;
            }

            u8 integer_param_index = 0;

            assert(call->parameters.size() <= emitter->parameter_registers.size()); // @Incomplete
            for (u32 i = 0; i < call->parameters.size(); ++i) {
                auto p = call->parameters[i];

                u8 param_reg = emitter->parameter_registers[integer_param_index];
                u8 int_param_reg = param_reg;
                integer_param_index += 1;

                u8 result = emit_load_of_value(emitter, object, code_section, p, int_param_reg, true);
                
                p->uses--;
            }

            bool has_internal_definition = function_target->blocks.size() != 0;

            if (object->use_absolute_addressing) {
                const u8 INDIRECT_CALL_REG = GP_R9;
                maybe_spill_register(emitter, &emitter->register_usage[INDIRECT_CALL_REG]);
                load_symbol_address(emitter, code_section, get_symbol_index(object, function_target), 0, INDIRECT_CALL_REG);
                blr(&emitter->function_buffer, INDIRECT_CALL_REG);
            } else {
                bl(&emitter->function_buffer, 0);

                /*
                emitter->function_buffer.append_byte(0xE8); // callq rip-relative
                // emitter->function_buffer.append_byte(ModRM(0b00, 0b000, 0b101));

                s32 *addr = emitter->function_buffer.allocate_unaligned<s32>();
                *addr = 0;

                if (has_internal_definition) {
                    emitter->rip_call_fixup_targets.emplace_back(emitter->function_buffer.size() - 4, function_target);
                } else {
                */
                    Relocation reloc;
                    reloc.type = Relocation::RIP_CALL;
                    reloc.offset = emitter->function_buffer.size() - 4;
                    reloc.symbol_index = get_symbol_index(object, function_target);
                    reloc.size = 4;
                    reloc.addend = 0; // @TODO
                    code_section->relocations.push_back(reloc);
                // }
            }

            return 0;
        }

        case INSTRUCTION_RETURN: {
            Instruction_Return *ret = static_cast<Instruction_Return *>(inst);

            /*
            if (ret->return_value) {
                u8 lhs_reg = maybe_get_instruction_register(ret->return_value);

                if (lhs_reg == 0xFF) lhs_reg = emit_load_of_value(emitter, object, code_section, ret->return_value, RAX, true);
                ret->return_value->uses--;

                if (lhs_reg != RAX) move_reg64_to_reg64(&emitter->function_buffer, lhs_reg, RAX);
            }

            if (!emitter->emitting_last_block) { // otherwise fallthrough to epilogue
                emitter->function_buffer.append_byte(0xE9);

                auto offset = emitter->function_buffer.size();
                emitter->epilogue_jump_target_fixups.push_back(offset);

                s32 *value = emitter->function_buffer.allocate_unaligned<s32>();
                emitter->epilogue_jump_target_fixup_pointers.push_back(value);
            }
            */
            break;
        }

        case INSTRUCTION_BRANCH: {
            Instruction_Branch *branch = static_cast<Instruction_Branch *>(inst);

/*
            // Spill all scratch registers at branches in case that we branch
            // to much earlier code that expects all registers to be free.
            // This may not totally be correct, but works for now. -josh 7 August 2020
            for (auto &reg : emitter->register_usage) {
                if (reg.machine_reg == RSP || reg.machine_reg == RBP) continue;

                maybe_spill_register(emitter, &reg);
            }

            if (branch->condition) {
                u8 cond = emit_load_of_value(emitter, object, code_section, branch->condition);
                sub_imm32_from_reg64(&emitter->function_buffer, cond, 0, branch->condition->value_type->size);

                maybe_spill_register(emitter, &emitter->register_usage[RAX]);
                
                emitter->function_buffer.append_byte(0x0F);
                emitter->function_buffer.append_byte(0x85); // jne if cond if true goto true block
                u32 *jne_disp = emitter->function_buffer.allocate_unaligned<u32>();
                
                auto failure_target = branch->failure_target;
                if (failure_target->type == VALUE_BASIC_BLOCK) {
                    *jne_disp = 5; // skip the next jmp instruction

                    Basic_Block *block = static_cast<Basic_Block *>(failure_target);

                    emitter->function_buffer.append_byte(0xE9);

                    // @Cutnpaste from emit_load_of_value
                    auto offset = emitter->function_buffer.size();
                    block->text_locations_needing_addr_fixup.push_back(offset);

                    u32 *value = emitter->function_buffer.allocate_unaligned<u32>();
                    block->text_ptrs_for_fixup.push_back(value);
                } else {
                    *jne_disp = 2; // skip the next jmp instruction

                    u8 fail_target = emit_load_of_value(emitter, object, code_section, branch->failure_target);
                    if (BIT3(fail_target)) {
                        *jne_disp += 1;
                        emitter->function_buffer.append_byte(REX(1, 0, 0, BIT3(fail_target)));
                    }

                    emitter->function_buffer.append_byte(0xFF); // jmp reg
                    emitter->function_buffer.append_byte(ModRM(0b11, 4, LOW3(fail_target)));
                }
            }

            Basic_Block *next_block = nullptr;
            // FIXME blocks should just know their insertion position
            // TODO maybe reoder blocks to optimize this
            for (u64 i = 0; i < function->blocks.size()-1; ++i) {
                if (function->blocks[i] == current_block) {
                    next_block = function->blocks[i+1];
                    break;
                }
            }

            if (branch->true_target != next_block) {
                auto true_target = branch->true_target;
                if (true_target->type == VALUE_BASIC_BLOCK) {
                    Basic_Block *block = static_cast<Basic_Block *>(true_target);

                    emitter->function_buffer.append_byte(0xE9);

                    // @Cutnpaste from emit_load_of_value
                    auto offset = emitter->function_buffer.size();
                    block->text_locations_needing_addr_fixup.push_back(offset);

                    u32 *value = emitter->function_buffer.allocate_unaligned<u32>();
                    block->text_ptrs_for_fixup.push_back(value);
                } else {
                    u8 target = emit_load_of_value(emitter, object, code_section, branch->true_target);

                    if (BIT3(target))
                        emitter->function_buffer.append_byte(REX(1, 0, 0, BIT3(target)));

                    emitter->function_buffer.append_byte(0xFF); // jmp reg
                    emitter->function_buffer.append_byte(ModRM(0b11, 4, LOW3(target)));
                }
            }
*/
            branch->true_target->uses--;
            if (branch->condition) branch->condition->uses--;
            if (branch->failure_target) branch->failure_target->uses--;
            break;
        }

        default: assert(false);
    }

    return 0;
}

static
Register make_reg(u8 machine_reg, bool is_free = true) {
    Register reg = {};
    reg.machine_reg = machine_reg;
    reg.is_free     = is_free;
    return reg;
}

/*
static
bool is_callee_saved(const Target &target, u8 reg) {
    if (target.is_win32())
        return (reg >= RBX && reg <= RDI) || (reg >= R12 && reg <= R15);
    else
        return reg == RBX || reg == RSP || reg == RBP || (reg >= R12 && reg <= R15);
}
*/

namespace josh {

void AArch64_emit_function(AArch64_Emitter *emitter, Linker_Object *object, Function *function) {
    if (function->intrinsic_id) return;
    if (function->uses == 0 && (function->blocks.size() == 0)) {
        // Function isn't used and is externally defined so we don't need
        // to emit anything, or even add this to the symbol table.
        return;
    }

    Section *code_section = emitter->code_section;

    emitter->function_buffer.clear();
    emitter->register_usage.clear();
    emitter->xmm_usage.clear();
    emitter->parameter_registers.clear();
    emitter->stack_size = 0;
    emitter->largest_call_stack_adjustment = 0;
    emitter->emitting_last_block = false;

    bool is_externally_defined = (function->blocks.size() == 0);

    if (is_externally_defined)
        assert(function->linkage == Function::Linkage::EXTERNAL);

    if (function->linkage == Function::Linkage::EXTERNAL) {
        u32 symbol_index = get_symbol_index(object, function);
        Symbol *sym = &object->symbol_table[symbol_index];
        sym->is_function = true;
        sym->is_externally_defined = is_externally_defined;

        if (!sym->is_externally_defined) {
            sym->section_number = code_section->section_number;
            sym->section_offset = emitter->code_section->data.size();
        }
    }

    if (is_externally_defined) return;

    emitter->function_text_locations[function] = emitter->code_section->data.size();

    for (u8 i = 0; i < GP_MAX; ++i)
        emitter->register_usage.push_back(make_reg(GP_R0 + i));

    emitter->register_usage.push_back(make_reg(SP, false));

    for (u8 i = 0; i <= GP_R7; ++i)
        emitter->parameter_registers.push_back(i);

    // TODO floating point registers

    size_t relocations_start = code_section->relocations.size();
    size_t rip_fixups_start  = emitter->rip_call_fixup_targets.size();
    size_t abs_fixups_start  = emitter->absolute_call_fixup_targets.size();

/*
    u32 reg_index = 0;
    for (u32 i = 0; i < function->arguments.size(); ++i) {
        Argument *arg = function->arguments[i];

        // TODO this is just a hacky way of handling small structs on System V, it is not totally correct
        if (auto str = arg->value_type->as<Struct_Type>(); str && str->size <= 16 && object->target.is_system_v()) {
            emitter->stack_size += str->size;

            Address_Info info = addr_register_disp(RBP, -emitter->stack_size);
            move_reg_to_memory(&emitter->function_buffer, emitter->parameter_registers[reg_index], info, str->size > 8 ? 8 : str->size);
            reg_index += 1;

            if (str->size > 8) {
                info.disp += 8;
                move_reg_to_memory(&emitter->function_buffer, emitter->parameter_registers[reg_index], info, str->size - 8);
                reg_index += 1;
            }

            arg->copied_to_stack_offset = -emitter->stack_size;
            continue;
        }

        claim_register(emitter, &emitter->register_usage[emitter->parameter_registers[reg_index]], arg);
        reg_index += 1;
    }
*/

    for (auto block : function->blocks) {
        for (auto inst : block->instructions) {
            if (inst->type == INSTRUCTION_ALLOCA) {
                auto _alloca = static_cast<Instruction_Alloca *>(inst);

                emitter->stack_size += (_alloca->alloca_type->size * _alloca->array_size);
                if ((emitter->stack_size % 8)) emitter->stack_size += 8 - (emitter->stack_size % 8);

                _alloca->stack_offset = -emitter->stack_size;
            }
        }
    }

    for (size_t i = 0; i < function->blocks.size(); ++i) {
        auto block = function->blocks[i];

        assert(block->has_terminator());
        block->text_location = emitter->function_buffer.size();

        emitter->emitting_last_block = (i == function->blocks.size()-1);

        for (auto inst : block->instructions) {
            emit_instruction(emitter, object, function, block, code_section, inst);
        }
    }

    // block text locations need to be offset by the end of the function prologue
    // relocation offsets need to be offset by end of the function prologue

    stp(&emitter->code_section->data, GP_R29, GP_R30, SP, -16);
    add_imm12_to_reg64(&emitter->code_section->data, GP_R29, SP, 0);

    emitter->stack_size += emitter->largest_call_stack_adjustment;
    // Ensure stack is 16-byte aligned.
    emitter->stack_size = ensure_aligned(emitter->stack_size, 16);
    assert((emitter->stack_size & (0xF)) == 0);
/*
    size_t num_push_pops = 0;
    bool pushed_rbp = false;

    if (emitter->stack_size != 0) {
        push(&emitter->code_section->data, RBP);
        num_push_pops += 1;
        pushed_rbp = true;
    }

    for (size_t i = 0; i < emitter->register_usage.size(); ++i) {
        auto reg = &emitter->register_usage[i];
        if (reg->used && is_callee_saved(object->target, reg->machine_reg)) {
            push(&emitter->code_section->data, reg->machine_reg);
            num_push_pops += 1;
        }
    }

    bool offset_push = false;
    if (num_push_pops % 2 == 0) {
        if (emitter->stack_size)
            emitter->stack_size += 8;
        else {
            offset_push = true;
            push(&emitter->code_section->data, RAX); // push a reg so we align stack properly without needing RBP pushed and RSP modified
        }
    }
*/
    if (emitter->stack_size != 0)
        sub_imm12_from_reg64(&emitter->code_section->data, SP, SP, emitter->stack_size);

/*
    // Touch stack pages from top to bottom
    // to release stack pages from the page guard system.
    // TODO we can probably simplify this by removing the loop
    // and emitting one mov instruction per stack page now that
    // we know the stack size post-function-body-generation
    // TODO do we need this for windows on arm ?
    if (object->target.is_win32()) {
        s32 *move_stack_size_to_rax = (s32 *)move_imm64_to_reg64(&emitter->code_section->data, emitter->stack_size, RAX, 4); // 4-byte immediate

        auto loop_start = emitter->code_section->data.size();
        sub_imm32_from_reg64(&emitter->code_section->data, RAX, 4096, 8);


        // @Cutnpaste from move_reg_to_memory
        auto dataptr = &emitter->code_section->data;
        dataptr->append_byte(REX(1, BIT3(RAX), 0, BIT3(RSP)));

        dataptr->append_byte(0x89);
        dataptr->append_byte(ModRM(MOD_INDIRECT_NO_DISP, LOW3(RAX), LOW3(RSP)));
        dataptr->append_byte(SIB(0, RAX, RSP));

        emitter->code_section->data.append_byte(0x0F);
        emitter->code_section->data.append_byte(0x8C); // jl if RAX < 0 break
        u32 *disp = emitter->code_section->data.allocate_unaligned<u32>();
        *disp = 5; // skip the next jmp instruction

        emitter->code_section->data.append_byte(0xE9); // jmp loop start
        disp = emitter->code_section->data.allocate_unaligned<u32>();
        *disp = (loop_start - emitter->code_section->data.size());
    }
*/

    size_t text_offset = emitter->code_section->data.size();

/*
    for (auto block : function->blocks) {
        for (u64 i = 0; i < block->text_locations_needing_addr_fixup.size(); ++i) {
            u64 location = block->text_locations_needing_addr_fixup[i];
            u32 *addr    = block->text_ptrs_for_fixup[i];

            *addr = static_cast<u32>((block->text_location - (location+4)));
        }
    }
*/
    emitter->code_section->data.append(&emitter->function_buffer);

    if (emitter->stack_size != 0)
        add_imm12_to_reg64(&emitter->code_section->data, SP, SP, emitter->stack_size);

/*
    size_t epilogue_start_offset = emitter->code_section->data.size();

    for (size_t i = 0; i < emitter->epilogue_jump_target_fixups.size(); ++i) {
        size_t location = text_offset + emitter->epilogue_jump_target_fixups[i];
        s32 *addr = emitter->epilogue_jump_target_fixup_pointers[i];

        *addr = static_cast<s32>(epilogue_start_offset - (location + 4));
    }

    if (offset_push) {
        pop(&emitter->code_section->data, RCX); // pop the stack alignment value into a caller-saved register
    }

    // reverse order pop
    for (size_t i = emitter->register_usage.size(); i > 0; --i) {
        auto reg = &emitter->register_usage[i-1];
        if (reg->used && is_callee_saved(object->target, reg->machine_reg))
             pop(&emitter->code_section->data, reg->machine_reg);
    }

    if (pushed_rbp)
        pop(&emitter->code_section->data, RBP);

*/

    ldp(&emitter->code_section->data, GP_R29, GP_R30, SP, 16, /*post_index*/true);
    ret(&emitter->code_section->data);


    for (size_t i = relocations_start; i < code_section->relocations.size(); ++i) {
        code_section->relocations[i].offset += text_offset;
    }

    for (size_t i = rip_fixups_start; i < emitter->rip_call_fixup_targets.size(); ++i) {
        emitter->rip_call_fixup_targets[i].first += text_offset;
    }

    for (size_t i = abs_fixups_start; i < emitter->absolute_call_fixup_targets.size(); ++i) {
        emitter->absolute_call_fixup_targets[i].first += text_offset;
    }
}

}